TODO

	Wie modellieren wir die Umwandlung von Baeumen zu Sequenzen?

		Implizite Darstellung: Gar nicht. Wir konstruieren Baeume, geben Parent Tokens in ein
		Netzwerk und setzen Kinder da ein, wo der Baum es fuer sinnvoll haelt. Problematisch:
		Wir muessten ganz viele Baeume offen halten und das noch irgendwie zuordnen koennen, praktisch
		nicht umsetzbar

		Explizite Darstellung: Komplette Umwandlung in Polish Notation, geht so in das Netz und
		das Ergebnis wird wieder eingeparsed

	Encoder fuer Token Embeddings sinnvoll?

	Padding von Input Daten bei rekurrentem Batchlearning

	Starttoken

Optimizer

	Ein Optimizer ist ein Verfahren, der anhand der Loss Function bzw. des berrechneten Loss die Gewichte bzw. Parameter des neuronalen 
	Netzes aktualisiert, z.B. Gradient Descent. Es gibt aber auch noch andere Optimizer.

Regularization

	Overfitting passiert, wenn manche Parameter besonders stark gewichtet werden. Regularization ist eine Funktion die auf die Loss 
	Function aufaddiert wird und besonders hohe Gewichte bestraft. Damit wird Overfitting vermieden.

Stochastic Gradient Descent

	Gradient Descent auf Batches

Adam 

	Adaptive Moment Estimation, ein Optimizer der kuerzlich berechnete Gradienten zum Teil miteinberechnet

Batches in RNNs

	"Said differently, whenever you train or test your LSTM, you first have to build your input matrix X of shape nb_samples, timesteps, input_dim where your batch size divides nb_samples. For instance, if nb_samples=1024 and batch_size=64, it means that your model will receive blocks of 64 samples, compute each output (whatever the number of timesteps is for every sample), average the gradients and propagate it to update the parameters vector."

	Feeding a sentence to a RNN:
	In general, for any Recurrent Neural Network (RNN), there is a concept of time instances (steps) corresponding to a time-series or sequence. The words in a sentence are fed to the network one at a time. So, each of the L words in a sentence is fed to the network one by one (step by step). When one sentence has been fed completely, the activations of the units in the hidden layer are reset. Then, the next sentence is fed, and so on.

	Feeding a batch of b sentences to a RNN:
	In step 1, first word of each of the b sentences (in a batch) is input in parallel. In step 2, second word of each of the b sentences is input in parallel. The parallelism is only for efficiency. Each sentence in a batch is handled in parallel, but the network sees one word of a sentence at a time and does the computations accordingly. All the computations involving the words of all sentences in a batch at a given time step are done in parallel.

	Der Hidden State wird fuer jedes Sample aus dem Batch separat gehalten und berechnet.

Feature

	Element des Input Vektors. Die Anzahl der Features ist gleich der Anzahl der Eingabe Dimensionen, also der Nodes der ersten Schicht 
	des neuronalen Netzes (Features also so ein wenig nach Eigenschaften die man mit einem Wert modellieren kann)

Timesteps vs Batchsize

Numpy Indexing und Tensor Funktionen

	[rows,columns]
	: - alle
	-1 letztes

	view() veraendert die Dimensionen 
	view(x) alles in eine Reihe mit Laenge x
	view(x,y) alles in eine 2dim Matrix
	view(-1,..) die -1 Dimension wird hergeleitet
	view(-1) alles in eine Reihe egal wie lang

	item() zieht ein Element aus einem Tensor

	to(device) erzeugt ein Tensor Objekt auf dem device

Autograd

	tensor.requires_grad = True
	tensor.backward()
	tensor.grad
	tensor.detach()

Pytorch

	net.parameters() alle lernbaren Parameter [layer, parameter]

	torch.nn nimmt nur Batches, erste Dimension von Tensoren immer 
	Anzahl der Batches

	optimizer = torch.optim.Adam(..)

	nn.Module.train(True) enabled oder disabled training, evaluation mode
	model.train()
	model.eval()

	to(device) Man muss das Model und die Daten auf das gleiche Device senden

Epoche

	one forward pass and one backward pass of all the training examples

Training des Generators

	1. Generiere Sequenz mit dem Generator
	2. Fuer jedes Token in der Sequenz, ermittle den Q(a,s) Wert wobei der 
	   State s die Sequenz vor dem Token ist, mit einer Monte Carlo Approximation,
	   d.h. wir werfen jedes Mal den Generator an und erzeugen mit Vorsequenz plus
	   Token N komplette Sequenzen, die dann bewertet werden
	3. Damit haben wir fuer jedes ausgewaehlte Token der Sequenz einen Wert zum 
	   Aktualisieren des Generators

	Wird ein RNN nach jedem Zeitschritt aktualisiert? Wenn nicht, wie wird der Loss dann korrekt
	zugeordnet?

	Ist es egal, ob einem bestimmten State, Action Paar ein Belohnungswert zugeordnet
	werden kann?

	
