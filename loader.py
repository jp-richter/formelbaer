from dataset import Dataset
from torch.utils.data import DataLoader

import config as cfg
import converter
import math
import os
import shutil
import generator
import datetime
import tokens
import torch


arxiv_data = None
oracle_data = None


def clear_synthetic_data_directory() -> None:
    """
    This function deletes all files in the synthetic data directory. The synthetic data directory serves as temporary
    store for data samples meant to be evaluated by the discriminating net. This function should be called after the
    evaluation or before the next evaluation to avoid evaluating the same data again.
    """

    shutil.rmtree(cfg.paths_cfg.synthetic_data)
    os.makedirs(cfg.paths_cfg.synthetic_data)


def save_pngs(samples, directory) -> None:
    """
    This function saves the given batch of samples produced by the generator to the given directory in the .png format.
    It also accepts lists of batches.

    :param samples: The samples that should be converted and saved. Needs to be a torch.Tensor of size (batch size,
        sequence length, one hot encoding length) or a list of such tensor objects.
    :param directory: The path to the directory to which the .png files will be written to.
    """

    converter.convert_to_png(samples, directory)


def save_sequences(samples, directory) -> None:
    """
    This function saves the sequences of integer ids for a given batch or list of sequences to the given directory. It
    is especially useful to see nodes of the syntax trees which will get ignored by the tree generation to assert
    their grammatical correctness. Also these integer id sequences allow to construct syntax trees whereas the saved
    png files only allow visual feedback of the generators performance. The sequences will be saved in text files as
    strings seperated by ',' f.e. 31,53,2,1 with every line representing a single sequence.

    :param samples: A batch in form of a tensor of size (batch size, sequence length, onehot encoding length) or a list
        of such tensors to save.
    :param directory: The path to the target directory the sequences.txt file will be written to.
    """

    with open(directory + '/sequences.txt', 'w') as f:

        sequences = []
        strings = []

        for sample in samples:
            sequence = []

            for onehot in sample:
                sequence.append(tokens.id(onehot))

            sequences.append(sequence)
            strings.append(', '.join(str(s) for s in sequence))

        all_sequences = '\n'.join(str(s) for s in sequences)
        f.write(all_sequences)


def get_loader_mixed_with_positives(synthetic_samples) -> DataLoader:
    """
    This function returns a torch Dataloader for a given set of synthetic samples which contains not only the synthetic
    samples but an equal amount of real samples either generated by an oracle or from the arxiv dataset. To use oracle
    data training set oracle in the configuration instance to True. The synthetic files get saved to the synthetic data
    directory temporarily and all files currently located in the directory will be deleted.

    :param synthetic_samples: A batch or list of batches of type tensor with size (batch size, sequence length, onehot
        encoding length) which can serve as negative training data for the discriminator.
    :return: Returns a torch dataloader with batch size = batchsize of current configuration, drop last = True and
        shuffle = True to make sure that the positive and negative samples do get mixed up.
    """

    clear_synthetic_data_directory()

    save_pngs(synthetic_samples, cfg.paths_cfg.synthetic_data)
    data = Dataset(cfg.paths_cfg.synthetic_data, label=cfg.app_cfg.label_synth)

    if cfg.app_cfg.oracle:
        data.merge(oracle_data.inorder(cfg.app_cfg.batchsize))
    else:
        data.merge(arxiv_data.inorder(cfg.app_cfg.batchsize))

    return DataLoader(data, batch_size=cfg.app_cfg.batchsize, drop_last=True, shuffle=True)


def load_single_batch(synthetic_samples) -> (torch.Tensor, torch.Tensor):
    """
    This function loads a single (images, labels) tuple from a given batch of synthetic samples. It does not return
    an iterator.

    :param synthetic_samples: A batch of type tensor with size (batch size, sequence length, one hot encoding length)
        to be loaded.
    :return: Returns a tuple of (images, labels) with types (tensor with size of input, tensor with size (batch size).
    """

    clear_synthetic_data_directory()

    save_pngs(synthetic_samples, cfg.paths_cfg.synthetic_data)
    data = Dataset(cfg.paths_cfg.synthetic_data, label=cfg.app_cfg.label_synth)
    loader = DataLoader(data, cfg.app_cfg.batchsize)

    return next(iter(loader))[0]  # (samples, labels)


def get_directory_with_timestamp() -> str:
    """
    This function creates a directory named with the current time in the main app directory specified by the current
    configuration. This is useful to get unique directory names when saving experiment data, as the time stamp includes
    milliseconds.

    :return: The path to the directory created.
    """

    directory = cfg.paths_cfg.app + '/' + str(datetime.datetime.now())[-15:]
    os.makedirs(directory)

    return directory


def make_directories() -> None:
    """
    This function creates all necessary directories and files used by the script. It should be called in the beginning
    of the script to avoid errors related to missing directories.
    """

    if not os.path.exists(cfg.paths_cfg.app):
        os.makedirs(cfg.paths_cfg.app)

    if not os.path.exists(cfg.paths_cfg.synthetic_data):
        os.makedirs(cfg.paths_cfg.synthetic_data)

    if not os.path.exists(cfg.paths_cfg.oracle_data):
        os.makedirs(cfg.paths_cfg.oracle_data)

    if not os.path.exists(cfg.paths_cfg.arxiv_data) and not cfg.app_cfg.oracle:
        raise ValueError('Either train with Oracle or provide training samples.')

    if not os.path.exists(cfg.paths_cfg.dump):
        open(cfg.paths_cfg.dump, 'w+')


def load_oracle_data() -> None:
    """
    This function loads the oracle dataset in case oracle is set to True in the current configuration. If not enough
    samples have been generated in the oracle data folder sufficient samples will be generated. Should not be called
    from the script, call load_data() instead.
    """

    global oracle_data

    nn_oracle = generator.Oracle()

    # generate oracle net with random weights
    if not os.path.exists(cfg.paths_cfg.oracle):
        nn_oracle.save(cfg.paths_cfg.oracle)
    else:
        nn_oracle.load(cfg.paths_cfg.oracle)

    # store samples from oracle distribution for adversarial training
    samplesize = len([name for name in os.listdir(cfg.paths_cfg.oracle_data)
                      if os.path.isfile(os.path.join(cfg.paths_cfg.oracle_data, name))])

    missing = max(cfg.app_cfg.oracle_samplesize - samplesize, 0)
    batch_num = math.ceil(missing / cfg.app_cfg.batchsize)

    if batch_num > 0:
        samples = generator.sample(nn_oracle, batch_num)
        save_pngs(samples, cfg.paths_cfg.oracle_data)

    oracle_data = Dataset(cfg.paths_cfg.oracle_data, label=cfg.app_cfg.label_arxiv)


def load_arxiv_data(log) -> None:
    """
    This function loads the arxiv dataset in case oracle is set to False in the current configuration. If not enough
    arxiv samples have been provided with respect to the current parameters an error will be thrown. Should not be
    called from the script, call load_data() instead.

    :param log: A logger instance of the python logging module to log potential errors.
    """

    global arxiv_data

    arxiv_data = Dataset(cfg.paths_cfg.arxiv_data, label=cfg.app_cfg.label_arxiv, recursive=True)

    print('{} samples loaded.'.format(len(arxiv_data)))
    log.log.info('{} samples loaded.'.format(len(arxiv_data)))

    provided = len(arxiv_data)
    needed = cfg.app_cfg.batchsize * cfg.app_cfg.d_steps * cfg.app_cfg.iterations

    message = '''Either provide more training samples or change parameters:
            Batchsize {}
            Discriminator Steps {}
            Iterations {}
            Positive Samples Needed {}
            Arxiv Samples Provided {}'''.format(
        cfg.app_cfg.batchsize,
        cfg.app_cfg.d_steps,
        cfg.app_cfg.iterations,
        needed,
        provided)

    if provided < needed:
        log.log.error(message)
        raise ValueError(message)


def load_data(log) -> None:
    """
    This function loads the dataset of real samples to train the discriminator with. It should be called at the
    beginning of the script. It assumes ray has already been initialized or errors will be thrown by the ray module.
    If oracle is set to True in the current configuration fake real samples of the oracle will be loaded instead of
    arxiv data.

    :param log: A logger instance of the python logging module to log potential errors.
    """

    if cfg.app_cfg.oracle:
        load_oracle_data()
    else:
        load_arxiv_data(log)
