import config

import torch
import generator
import discriminator
import loader
import log
import tree


def collect_reward(nn_discriminator, batch):
    """
    This function calculates the rewards given a batch of onehot sequences with the given discriminator. The rewards
    will be the probability that the sequences are no synthetic predicted by the discriminator.

    :param nn_discriminator: The discriminator which predictions the rewards are based on.
    :param batch: The batch of sequences generated by the generator.
    :return: Returns a tensor of size (batchsize, 1).
    """

    images = loader.prepare_batch(batch)
    output = nn_discriminator(images)
    reward = torch.empty((batch.shape[0], 1), device=config.general.device)

    for r in range(output.shape[0]):
        reward[r][0] = 1 - output[r]

    # DEBUG: print formulars with reward > 0.5
    # transform = torchvision.transforms.ToPILImage()
    #
    # for i, r in enumerate(reward):
    #     if r.item() > 0.5:
    #         sample = batch[i]
    #         sample = [tokens.id(onehot) for onehot in sample]
    #         sample = tree.to_tree(sample)
    #         sample = sample.latex()
    #         print('Reward {} for sample {}'.format(r.item(), sample))
    #
    #         image = transform(images[i])
    #         image.show()
    #         image.save('/home/richter2/formelbaer-data/{}.png'.format(sample))
    #
    #         for i, sample in enumerate(batch):
    #             sample = batch[i]
    #             sample = [tokens.id(onehot) for onehot in sample]
    #             sample = tree.to_tree(sample)
    #             sample = sample.latex()
    #             print(sample)
    #
    #         return

    return reward


def adversarial_generator(nn_policy, nn_rollout, nn_discriminator, epoch, step) -> None:
    """
    The training loop of the generating policy net.

    :param epoch: The current iteration of the adversarial training for the logging module.
    :param nn_policy: The policy net that is the training target.
    :param nn_rollout: The rollout net that is used to complete unfinished sequences for estimation of rewards.
    :param nn_discriminator: The CNN that estimates the probability that the generated sequences represent the real
        data distribution, which serve as reward for the policy gradient training of the policy net.
    """

    nn_rollout.set_parameters_to(nn_policy)
    nn_policy.train()
    nn_rollout.eval()
    nn_discriminator.eval()

    batch_size = config.general.batch_size
    sequence_length = config.general.sequence_length
    montecarlo_trials = config.general.montecarlo_trials

    batch, hidden = nn_policy.initial()

    for length in range(sequence_length):

        # generate a single next token given the sequences generated so far
        batch, hidden = generator.step(nn_policy, batch, hidden, save_prob=True)
        q_values = torch.empty([batch_size, 0], device=config.general.device)

        # compute the Q(token,subsequence) values with monte carlo approximation
        if not batch.shape[1] < sequence_length:
            for _ in range(montecarlo_trials):
                samples = generator.rollout(nn_rollout, batch, hidden)
                reward = collect_reward(nn_discriminator, samples)
                q_values = torch.cat([q_values, reward], dim=1)
        else:
            reward = collect_reward(nn_discriminator, batch)
            q_values = torch.cat([q_values, reward], dim=1)

        # average the reward over all trials
        q_values = torch.mean(q_values, dim=1)
        nn_policy.rewards.append(q_values)

        generator.policy_gradient_update(nn_policy)  # TODO comment out to reward like in SeqGAN
        batch, hidden = (batch.detach(), hidden.detach())  # TODO comment out to reward like in SeqGAN

    # generator.policy_gradient_update(nn_policy)  # TODO comment in to reward like in SeqGAN
    log.generator_loss(nn_policy, epoch, step)

    # DEBUG PRINT
    batch = batch[-3:]
    trees = tree.to_trees(batch.tolist())
    latexs = [t.latex() for t in trees]

    for l in latexs:
        print('Example Formular: ' + l)
        log.log.info('Example Formular: ' + l)


def adversarial_discriminator(nn_discriminator, nn_generator, nn_oracle, d_steps, d_epochs, epoch) -> None:
    """
    The training loop of the discriminator net.

    :param epoch: The current iteration of the adversarial training for the logging module.
    :param d_steps: The amount of steps the discriminator should be trained in one adversarial cycle.
    :param nn_generator: The policy net which generates the synthetic data the CNN gets trained to classify.
    :param nn_discriminator: The CNN that outputs an estimation of the probability that a given data point was generated
        by the policy network.
    :param nn_oracle: If the script uses oracle training fake real samples will be generated by the oracle net.
    :param d_epochs: The amount of epochs the discriminator trains per d step. In case of oracle training a samplesize
        can be specified and one epoch will contain the samplesize of positive and an equal amount of negative samples.
        In case the discriminator gets trained on arxiv data an upper limit of real samples can be specified and one
        epoch will contain the limit of real and an equal amount of generated samples.
    """

    nn_discriminator.reset()
    nn_discriminator.train()
    nn_generator.eval()

    num_samples = config.general.num_real_samples * 2 * d_steps  # equal amount of generated data
    data_loader = loader.prepare_loader(num_samples, nn_generator, nn_oracle)

    debug = []
    count = 0

    for d_epoch in range(d_epochs):
        for images, labels in data_loader:
            images = images.to(config.general.device)
            labels = labels.to(config.general.device)

            nn_discriminator.optimizer.zero_grad()
            outputs = nn_discriminator(images)

            # output[:,0] P(x ~ real)
            # output[:,1] P(x ~ synthetic)

            count += images.shape[0]
            if len(data_loader.dataset) - count <= config.general.batch_size:
                debug = [(str(out.item()), str(lab.item())) for (out, lab) in zip(outputs, labels)]

            loss = nn_discriminator.criterion(outputs, labels.float())
            loss.backward()
            nn_discriminator.optimizer.step()

            nn_discriminator.running_loss += loss.item()
            nn_discriminator.loss_divisor += 1
            nn_discriminator.running_acc += torch.sum((outputs > 0.5) == (labels == 1)).item()
            nn_discriminator.acc_divisor += outputs.shape[0]

        log.discriminator_loss(nn_discriminator, epoch, d_epoch)

    # DEBUG PRINT
    print('---')
    print('DEBUG: DISCRIMINATOR PREDICTIONS')
    log.log.info('DEBUG: DISCRIMINATOR PREDICTIONS')

    for output, label in debug:
        print('Prediction ' + output + ' Label ' + label)
        log.log.info('Prediction ' + output + ' Label ' + label)


def training() -> None:
    """
    The main loop of the script. To change parameters of the adversarial training parameters should not be changed here.
    Overwrite the configuration variables in config.py instead and start the adversarial training again.
    """

    loader.initialize()

    nn_discriminator = discriminator.Discriminator().to(config.general.device)
    nn_policy = generator.Policy().to(config.general.device)
    nn_rollout = generator.Policy().to(config.general.device)
    nn_oracle = generator.Oracle().to(config.general.device)

    nn_discriminator.criterion = torch.nn.BCELoss()
    nn_oracle.criterion = torch.nn.NLLLoss()
    nn_discriminator.optimizer = torch.optim.Adam(nn_discriminator.parameters(), lr=config.discriminator.learnrate)
    nn_policy.optimizer = torch.optim.Adam(nn_policy.parameters(), lr=config.generator.learnrate)

    # start adversarial training
    d_steps = config.general.d_steps
    g_steps = config.general.g_steps
    a_epochs = config.general.total_epochs
    d_epochs = config.general.d_epochs

    for epoch in range(a_epochs):

        # train D
        adversarial_discriminator(nn_discriminator, nn_policy, nn_oracle, d_steps, d_epochs, epoch)

        # train G
        for step in range(g_steps):
            adversarial_generator(nn_policy, nn_rollout, nn_discriminator, epoch, step)

        # increase D performance every 20th step
        if not epoch == 0 and epoch%20 == 0 and config.general.num_real_samples < 10000:
            config.general.num_real_samples += 1000

    loader.finish(nn_policy, nn_discriminator, nn_oracle)


def application() -> None:
    training()
    loader.shutdown()


if __name__ == '__main__':
    application()
