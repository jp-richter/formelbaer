from nn_policy import Policy, Oracle
from nn_discriminator import Discriminator

import config as cfg

import math
import torch
import generator
import discriminator
import loader
import log
import os
import multiprocessing


def generator_training(nn_policy, nn_rollout, nn_discriminator, nn_oracle, g_opt, o_crit):
    """ The training loop of the generating policy net.

    nn_policy :: nn_policy.Policy 
        The policy net that is the training target.
    nn_rollout :: nn_policy.Policy 
        The rollout net that is used to complete unfinished sequences for estimation of rewards.
    nn_discriminator :: nn_discriminator.Discriminator 
        The CNN that estimates the probability that the generated sequences represent the real
        data distribution, which serve as reward for the policy gradient training of the policy net.
    nn_oracle :: nn_policy.Oracle
        A policy net which gets initialized with high variance parameters and serves as fake real
        distribution to analyze the performance of the model even when no comparisons to other
        models can be made.
    g_opt :: torch.optimizer
        The optimizer of the policy net.
    o_crit :: torch.nn.loss
        The performance criterion for the oracle net."""

    nn_policy.train()
    nn_rollout.eval()

    for _ in range(cfg.app_cfg.g_steps):
        batch, hidden = nn_policy.initial()

        for length in range(cfg.app_cfg.seq_length):
            batch, hidden = generator.step(nn_policy, batch, hidden, nn_oracle, o_crit, save_prob=True)
            q_values = torch.empty([cfg.app_cfg.batchsize, 0])

            if batch.shape[1] < cfg.app_cfg.seq_length:

                for _ in range(cfg.app_cfg.montecarlo_trials):
                    samples = generator.rollout(nn_rollout, batch, hidden)
                    samples = loader.load_single_batch(samples)
                    reward = discriminator.evaluate_single_batch(nn_discriminator, samples)

                    q_values = torch.cat([q_values, reward], dim=1)

            else:
                # calculate reward for last step without montecarlo approximation
                samples = loader.load_single_batch(batch)
                reward = discriminator.evaluate_single_batch(nn_discriminator, samples)
                q_values = torch.cat([q_values, reward], dim=1)

            # average the reward over 
            q_values = torch.mean(q_values, dim=1)
            generator.reward(nn_policy, q_values)

        generator.update(nn_policy, g_opt)


def discriminator_training(nn_discriminator, nn_generator, d_opt, d_crit):
    """The training loop of the discriminator net.

    nn_discriminator :: nn_discriminator.Discriminator
        The CNN that outputs an estimation of the probability that a given data point was
        generated by the policy network.
    nn_generator :: nn_policy.Policy
        The policy net which generates the synthetic data the CNN gets trained to classify.
    d_opt :: torch.optimizer
        The optimizer of the CNN.
    d_crit :: torch.nn.loss
        The performance criterion for the CNN."""

    nn_discriminator.train()
    nn_generator.eval()

    for _ in range(cfg.app_cfg.d_steps):

        synthetic = generator.sample(nn_generator, 1)
        torch_loader = loader.get_pos_neg_loader(synthetic)
        discriminator.update(nn_discriminator, d_opt, d_crit, torch_loader)


def adversarial_training():
    """The main loop of the script. To change parameters of the adversarial training parameters
    should not be changed here. Overwrite the configuration variables in config.py instead and
    start the adversarial training again."""

    # INITIALIZATION

    log.start_loading_data()
    loader.initialize(log) # must be called first / loads data & makes directories
    log.finish_loading_data()

    nn_discriminator = Discriminator()
    nn_policy = Policy()
    nn_rollout = Policy()
    nn_oracle = Oracle() 

    if cfg.app_cfg.oracle:
        nn_oracle.load(cfg.paths_cfg.oracle)

    d_opt = torch.optim.Adam(nn_discriminator.parameters(), lr=cfg.d_cfg.learnrate)
    d_crit = torch.nn.BCELoss()
    g_opt = torch.optim.Adam(nn_policy.parameters(), lr=cfg.g_cfg.learnrate)
    o_crit = torch.nn.KLDivLoss()

    # START ADVERSARIAL TRAINING
    
    log.start_experiment()

    for i in range(cfg.app_cfg.iterations):
        nn_rollout.set_parameters_to(nn_policy)

        discriminator_training(nn_discriminator, nn_rollout, d_opt, d_crit)
        generator_training(nn_policy, nn_rollout, nn_discriminator, nn_oracle, g_opt, o_crit)

        log.write(i+1, nn_policy, nn_discriminator, nn_oracle, printout=True)

    # FINISH EXPERIMENT AND WRITE LOGS

    directory = loader.get_experiment_directory()
    nn_policy.save(directory + '/policy_net.pt')
    nn_discriminator.save(directory + '/discriminator_net.pt')
    nn_oracle.save(directory + '/oracle_net.pt')

    log.finish_experiment(directory)

    evaluation = generator.sample(nn_policy, math.ceil(100 / cfg.app_cfg.batchsize))
    
    os.makedirs(directory + '/pngs')
    os.makedirs(directory + '/sequences')
    loader.save_pngs(evaluation, directory + '/pngs')
    loader.save_sequences(evaluation, directory + '/sequences')


def application():

    experiment = cfg.AppConfig(

    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'),

    iterations = 2,
    d_steps = 2, # (*2) due to computational cost reasons
    g_steps = 1,
    seq_length = 10, # 15
    montecarlo_trials = 10, # 15
    batchsize = multiprocessing.cpu_count(), # computational cost reasons

    oracle = True,
    oracle_samplesize = 100,

    label_synth = 1,
    label_arxiv = 0

    )

    cfg.app_cfg = experiment
    adversarial_training()

    experiment = cfg.AppConfig(

    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'),

    iterations = 2,
    d_steps = 2, # (*2) due to computational cost reasons
    g_steps = 1,
    seq_length = 30, # 15
    montecarlo_trials = 10, # 15
    batchsize = multiprocessing.cpu_count(), # computational cost reasons

    oracle = True,
    oracle_samplesize = 100,

    label_synth = 1,
    label_arxiv = 0

    )

    cfg.app_cfg = experiment
    adversarial_training()

    experiment = cfg.AppConfig(

    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'),

    iterations = 2,
    d_steps = 2, # (*2) due to computational cost reasons
    g_steps = 1,
    seq_length = 20, # 15
    montecarlo_trials = 10, # 15
    batchsize = multiprocessing.cpu_count(), # computational cost reasons

    oracle = True,
    oracle_samplesize = 100,

    label_synth = 1,
    label_arxiv = 0

    )

    cfg.app_cfg = experiment
    adversarial_training()


if __name__ == '__main__':
      application()  
