from config import config, paths, Config
from discriminator import Discriminator
from progress.bar import Bar
from helper import store
from collections import Counter

import torch
import generator
import loader
import tree
import os
import tokens


def store_results(rewards, loss, reward_without_log_prob, entropy, policy):
    prediction = sum(rewards[-1]) / config.batch_size
    store.add('Generator Prediction', prediction.item(), store.PLOTTABLE)
    store.add('Generator Loss', loss.item(), store.PLOTTABLE)
    store.add('Generator Reward', reward_without_log_prob.item(), store.PLOTTABLE)
    store.add('Generator Entropy ', entropy.item(), store.PLOTTABLE)

    mean_policies = store.rmget('Generator Policy Means Temp')
    mean_policies = torch.mean(torch.stack(mean_policies, dim=0), dim=0)
    store.add('Generator Average Policy', mean_policies.tolist())

    sampled_actions = store.rmget('Generator Sampled Actions Temp')
    action_counts = Counter([i.item() for s in sampled_actions for i in s])
    sampled_actions = [action_counts[index] if index in action_counts.keys() else 0 for index in range(tokens.count())]
    store.add('Generator Sampled Actions', sampled_actions)

    policy.save('{}/policies/{}'.format(store.folder, store.get('Policy Step')))
    store.add('Policy Step', store.rmget('Policy Step')[0] + 1)


def policy_gradient(policy):
    policy.optimizer.zero_grad()

    # weight state action values by log probability of action
    total = torch.zeros(config.batch_size, device=config.device)
    reward_with_log_prob = torch.zeros(config.batch_size, device=config.device)
    reward_without_log_prob = torch.zeros(config.batch_size, device=config.device)

    log_probs = store.rmget('Generator Log Probs Temp')
    rewards = store.rmget('Generator Rewards Temp')

    assert len(rewards) == len(log_probs)
    assert all(tensor.size() == (config.batch_size,) for tensor in log_probs)
    assert all(tensor.size() == (config.batch_size,) for tensor in rewards)

    for log_prob, reward in zip(log_probs, rewards):
        total = total + (reward - config.g_baseline)
        reward_with_log_prob = reward_with_log_prob + (log_prob * total)
        reward_without_log_prob = reward_without_log_prob + total

    # average over batchsize
    reward_without_log_prob = torch.sum(reward_without_log_prob) / config.batch_size
    reward_with_log_prob = torch.sum(reward_with_log_prob) / config.batch_size

    # negate for gradient descent and substract entropy
    entropies = store.rmget('Generator Entropy Means Temp')
    entropy = 0.005 * sum(entropies) / len(entropies)

    loss = - (reward_with_log_prob + entropy)
    loss.backward()
    policy.optimizer.step()

    store_results(rewards, loss, reward_without_log_prob, entropy, policy)


def collect_reward(discriminator, batch):
    """
    This function calculates the rewards given a batch of onehot sequences with the given discriminator. The rewards
    will be the probability that the sequences are no synthetic predicted by the discriminator.

    :param discriminator: The discriminator which predictions the rewards are based on.
    :param batch: The batch of sequences generated by the generator.
    :return: Returns a tensor of size (batchsize, 1).
    """

    images = loader.prepare_batch(batch)
    output = discriminator(images)
    reward = torch.empty((batch.shape[0], 1), device=config.device)

    # TODO punish atomic expressions

    for r in range(output.shape[0]):
        reward[r][0] = 1 - output[r]

    return reward


def adversarial_generator(policy, rollout, discriminator, adversarial_step, g_steps):
    rollout.set_parameters_to(policy)
    policy.train()
    rollout.eval()
    discriminator.eval()

    for step in range(g_steps):
        batch, hidden = policy.initial()

        for length in range(config.sequence_length):

            # generate a single next token given the sequences generated so far
            batch, hidden = generator.step(policy, batch, hidden, save_prob=True)
            q_values = torch.empty([config.batch_size, 0], device=config.device)

            # compute the Q(token,subsequence) values with monte carlo approximation
            if not batch.shape[1] < config.sequence_length:
                for _ in range(config.montecarlo_trials):
                    samples = generator.rollout(rollout, batch, hidden)
                    reward = collect_reward(discriminator, samples)
                    q_values = torch.cat([q_values, reward], dim=1)
            else:
                reward = collect_reward(discriminator, batch)
                q_values = torch.cat([q_values, reward], dim=1)

            # average the reward over all trials
            q_values = torch.mean(q_values, dim=1)
            store.add('Generator Rewards Temp', q_values)

            # generator.policy_gradient_update(policy)  # TODO comment out to reward like in SeqGAN
            # batch, hidden = (batch.detach(), hidden.detach())  # TODO comment out to reward like in SeqGAN

        store.add('Formular Examples', ', '.join(tree.to_latex(batch[-3:].tolist())))
        policy_gradient(policy)


def adversarial_discriminator(discriminator, policy, adversarial_step, d_steps, d_epochs):
    discriminator.reset()
    discriminator.train()
    policy.eval()

    num_samples = config.num_real_samples * 2 * d_steps  # equal amount of generated data
    data_loader = loader.prepare_loader(num_samples, policy)

    for epoch in range(d_epochs):
        for images, labels in data_loader:
            images = images.to(config.device)
            labels = labels.to(config.device)

            discriminator.optimizer.zero_grad()
            outputs = discriminator(images)

            # output[:,0] P(x ~ real)
            # output[:,1] P(x ~ synthetic)

            loss = discriminator.criterion(outputs, labels.float())
            loss.backward()
            discriminator.optimizer.step()

            store.add('Discriminator Loss Temp', loss.item())
            store.add('Discriminator Acc Temp', torch.sum((outputs > 0.5) == (labels == 1)).item() / outputs.shape[0])

        loss = store.rmget('Discriminator Loss Temp')
        acc = store.rmget('Discriminator Acc Temp')
        store.add('Discriminator Loss', sum(loss) / len(loss), store.PLOTTABLE)
        store.add('Discriminator Acc', sum(acc) / len(acc), store.PLOTTABLE)


def training(discriminator, policy, rollout):
    """
    The main loop of the script. To change parameters of the adversarial training parameters should not be changed here.
    Overwrite the configuration variables in config.py instead and start the adversarial training again.
    """

    print('Starting adversarial training..')
    progress = Bar('Iteration Progress', max=config.adversarial_steps)
    for adversarial_step in range(config.adversarial_steps):
        progress.next()

        adversarial_discriminator(discriminator, policy, adversarial_step, config.d_steps, config.d_epochs)
        adversarial_generator(policy, rollout, discriminator, adversarial_step, config.g_steps)

        if not adversarial_step == 0 and adversarial_step % 10 == 0:
            policy.save(paths.policies)

        if not adversarial_step == 0 and adversarial_step % 20 == 0 and config.general.num_real_samples < 10000:
            config.num_real_samples += 1000

    progress.finish()
    print('Finished training and saving results.')
    return discriminator, policy


def initialize():
    """
    Setup neural networks and tensorboard logging.
    """

    discriminator = Discriminator().to(config.device)
    policy = generator.Policy().to(config.device)
    rollout = generator.Policy().to(config.device)

    discriminator.criterion = torch.nn.BCELoss()
    discriminator.optimizer = torch.optim.Adam(discriminator.parameters(), lr=config.d_learnrate)
    policy.optimizer = torch.optim.Adam(policy.parameters(), lr=config.g_learnrate)

    hyperparameter = {k: v for k, v in config.__dict__.items() if not v == config.device}

    notes = '''
    0.1 Added Multipages   
    0.2 Resetting D Weights After Each Step
    0.3 Set Batchsize to cores
    0.4 Switched to update policy after each step in a sequence
    0.5 Switched learningrate from 0.05 to 0.001
    0.6 Switched back to update after sequence, batchsize to 4*cores
    0.7 Increment real samples D trains by 2000 Samples every 20 Epochs (max 10.000)
    0.8 Loss + Entropy * beta - Gamma 1 - Bias - switched to 1000 samples per epoch
    0.9 entropy beta 0.01 -> 0.005
    '''

    store.setup(loader.make_directory_with_timestamp(), hyperparameter, notes)
    store.add('Policy Step', 0)
    os.makedirs('{}/policies'.format(store.folder))

    return discriminator, policy, rollout


def application() -> None:
    loader.initialize()

    # training
    discriminator, policy, rollout = initialize()
    training(discriminator, policy, rollout)

    loader.finish(policy, discriminator)


if __name__ == '__main__':
    application()
