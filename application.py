from config import config, paths
from discriminator import Discriminator
from collections import Counter
from progress.bar import Bar

import torch
import generator
import loader
import tree
import info
import tokens


# TODO kein smoothing (curve) fuer median action, vlt stattdessen histogram?
# TODO bei tensorboard manche sachen out of bounds?
# TODO mein eigenes experiment info speichert die daten nicht


def policy_tensorboard(policy, adversarial_step, g_step, batch, board):
    """

    :param policy:
    :param adversarial_step:
    :param g_step:
    :param batch:
    :param board:
    """

    loss = sum(policy.average_losses) / len(policy.average_losses)
    prediction = sum(policy.average_predictions) / len(policy.average_predictions)
    reward = sum(policy.average_rewards) / len(policy.average_rewards)
    entropy = sum(policy.average_entropies) / len(policy.average_entropies)

    policies = torch.mean(torch.stack(policy.average_policies, dim=0), dim=0)
    action_dict = Counter([id.item() for sample in policy.sampled_actions for id in sample])  # returns {unique values: counts}
    actions = [action_dict[index] if index in action_dict.keys() else 0 for index in range(tokens.count())]

    del policy.average_losses[:]
    del policy.average_predictions[:]
    del policy.average_rewards[:]
    del policy.average_entropies[:]
    del policy.average_policies[:]
    del policy.sampled_actions[:]

    board.add_scalar('Generator/Loss', loss, adversarial_step + g_step)
    board.add_scalar('Generator/Prediction', prediction, adversarial_step + g_step)
    board.add_scalar('Generator/Reward', reward, adversarial_step + g_step)
    board.add_scalar('Generator/Entropy', entropy, adversarial_step + g_step)
    board.add_scalars('Generator/Loss-Reward', {'Loss': loss, 'Reward': reward}, adversarial_step + g_step)

    board.add_histogram('Generator/Actions', torch.tensor(actions), adversarial_step + g_step)
    board.add_histogram('Generator/AveragePolicy', policies, adversarial_step + g_step)  # TODO

    examples = ', '.join(tree.to_latex(batch[-3:].tolist()))
    board.add_text('Generator/Formulars', examples, adversarial_step + g_step)


def discriminator_tensorboard(discriminator, adversarial_step, d_epochs, d_epoch, board):
    """

    :param discriminator:
    :param adversarial_step:
    :param d_epochs:
    :param d_epoch:
    """

    loss = sum(discriminator.loss) / len(discriminator.loss)
    acc = sum(discriminator.acc) / len(discriminator.acc)

    del discriminator.loss[:]
    del discriminator.acc[:]

    board.add_scalar('Discriminator/Loss', loss, adversarial_step * d_epochs + d_epoch)
    board.add_scalar('Discriminator/Accuracy', acc, adversarial_step * d_epochs + d_epoch)


def collect_reward(nn_discriminator, batch):
    """
    This function calculates the rewards given a batch of onehot sequences with the given discriminator. The rewards
    will be the probability that the sequences are no synthetic predicted by the discriminator.

    :param nn_discriminator: The discriminator which predictions the rewards are based on.
    :param batch: The batch of sequences generated by the generator.
    :return: Returns a tensor of size (batchsize, 1).
    """

    images = loader.prepare_batch(batch)
    output = nn_discriminator(images)
    reward = torch.empty((batch.shape[0], 1), device=config.device)

    # TODO punish atomic expressions

    for r in range(output.shape[0]):
        reward[r][0] = 1 - output[r]

    return reward


def adversarial_generator(policy, rollout, discriminator, adversarial_step, g_steps, board):

    rollout.set_parameters_to(policy)
    policy.train()
    rollout.eval()
    discriminator.eval()

    for step in range(g_steps):
        batch, hidden = policy.initial()

        for length in range(config.sequence_length):

            # generate a single next token given the sequences generated so far
            batch, hidden = generator.step(policy, batch, hidden, save_prob=True)
            q_values = torch.empty([config.batch_size, 0], device=config.device)

            # compute the Q(token,subsequence) values with monte carlo approximation
            if not batch.shape[1] < config.sequence_length:
                for _ in range(config.montecarlo_trials):
                    samples = generator.rollout(rollout, batch, hidden)
                    reward = collect_reward(discriminator, samples)
                    q_values = torch.cat([q_values, reward], dim=1)
            else:
                reward = collect_reward(discriminator, batch)
                q_values = torch.cat([q_values, reward], dim=1)

            # average the reward over all trials
            q_values = torch.mean(q_values, dim=1)
            policy.rewards.append(q_values)

            # generator.policy_gradient_update(policy)  # TODO comment out to reward like in SeqGAN
            # batch, hidden = (batch.detach(), hidden.detach())  # TODO comment out to reward like in SeqGAN

        generator.policy_gradient_update(policy)  # TODO comment in to reward like in SeqGAN
        policy_tensorboard(policy, adversarial_step, step, batch, board)


def adversarial_discriminator(discriminator, policy, adversarial_step, d_steps, d_epochs, board):
    discriminator.reset()
    discriminator.train()
    policy.eval()

    num_samples = config.num_real_samples * 2 * d_steps  # equal amount of generated data
    data_loader = loader.prepare_loader(num_samples, policy)

    for epoch in range(d_epochs):
        for images, labels in data_loader:
            images = images.to(config.device)
            labels = labels.to(config.device)

            discriminator.optimizer.zero_grad()
            outputs = discriminator(images)

            # output[:,0] P(x ~ real)
            # output[:,1] P(x ~ synthetic)

            loss = discriminator.criterion(outputs, labels.float())
            loss.backward()
            discriminator.optimizer.step()

            discriminator.loss.append(loss.item())
            discriminator.acc.append(torch.sum((outputs > 0.5) == (labels == 1)).item() / outputs.shape[0])

        discriminator_tensorboard(discriminator, adversarial_step, d_epochs, epoch, board)


def training(discriminator, policy, rollout, board):
    """
    The main loop of the script. To change parameters of the adversarial training parameters should not be changed here.
    Overwrite the configuration variables in config.py instead and start the adversarial training again.
    """

    print('Starting adversarial training..')
    progress = Bar('Iteration Progress', max=config.adversarial_steps)
    for adversarial_step in range(config.adversarial_steps):

        progress.next()

        adversarial_discriminator(discriminator, policy, adversarial_step, config.d_steps, config.d_epochs, board)
        adversarial_generator(policy, rollout, discriminator, adversarial_step, config.g_steps, board)

        if not adversarial_step == 0 and adversarial_step%10 == 0:
            policy.save(paths.policies)

        if not adversarial_step == 0 and adversarial_step%20 == 0 and config.general.num_real_samples < 10000:
            config.num_real_samples += 1000

    progress.finish()
    print('Finished training and saving results.')
    return discriminator, policy


def initialize(experiment):
    """
    Setup neural networks and tensorboard logging.
    """

    discriminator = Discriminator().to(config.device)
    policy = generator.Policy().to(config.device)
    rollout = generator.Policy().to(config.device)

    discriminator.criterion = torch.nn.BCELoss()
    discriminator.optimizer = torch.optim.Adam(discriminator.parameters(), lr=config.d_learnrate)
    policy.optimizer = torch.optim.Adam(policy.parameters(), lr=config.g_learnrate)

    hyperparameter = config.__dict__  # TODO fix, kein step hparams
    experiment['hyperparameter'] = hyperparameter

    notes = '''
    0.1 Added Multipages   
    0.2 Resetting D Weights After Each Step
    0.3 Set Batchsize to cores
    0.4 Switched to update policy after each step in a sequence
    0.5 Switched learningrate from 0.05 to 0.001
    0.6 Switched back to update after sequence, batchsize to 4*cores
    0.7 Increment real samples D trains by 2000 Samples every 20 Epochs (max 10.000)
    0.8 Loss + Entropy * beta - Gamma 1 - Bias - switched to 1000 samples per epoch
    0.9 entropy beta 0.01 -> 0.005
    '''
    experiment

    return discriminator, policy, rollout


def application() -> None:
    loader.initialize()

    # setup logging
    experiment = info.ExperimentInfo(loader.make_directory_with_timestamp())

    # training
    discriminator, policy, rollout = initialize(experiment)
    training(discriminator, policy, rollout, experiment)

    loader.finish(policy, discriminator, experiment)
    loader.shutdown()


if __name__ == '__main__':
    application()
